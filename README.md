# Michaverse AI Playground

<div align="center">
<img src="Hero.jpg" 
     alt="Michaverse AI Playground" 
     style="width: 100%;
            height: auto;">
</div>

## ğŸ¬ Demo Videos

**Instagram**: [Summary](https://www.instagram.com/p/DLwDIhvK2zw/)

**YouTube**: [Full Breakdown](https://www.youtube.com/watch?v=X06M-wwQNo4)

---

## What is this?

This is a modular, self-built AI playground and interface system built entirely in JavaScript and Python. It integrates multiple AI models in a seamless, interactive UI and allows:

Locals (need at least 8GB of VRAM and a NVidia Card)
- Transcription via [Whisper](https://github.com/openai/whisper)
- Text-to-speech via [Zonos](https://github.com/sdbds/Zonos-for-windows)
- Text-to-speech via [Chatterbox](https://github.com/resemble-ai/chatterbox)

APIs
- LLM interaction via OpenAI GPT4.1 & GPT4.1-mini
- Image generation via Google Imagen and OpenAI DALLÂ·E
- Translation via Google Translate

Functionality
- Combined pipelines (STT + Translate + LLM + TTS (with sampled Voices) + Image)
- Personality & voice mapping chosen by LLM
- Seesion & LongTerm Memory handling by LLM
- LLM can initiate Conversations
- Audio normalization via FFmpeg
- Responsive layout with resizable panels

---

## Architecture

- **Frontend**: Vanilla JS + HTML/CSS
- **Backend**: Node.js + Python (via spawn)
- **Models**: OpenAI GPT, Whisper, Zonos, Chatterbox, FFmpeg, Google Imagen, OpenAI DALLÂ·E, Google Translate

---

## Installation

### 1. Clone the repo
```bash
git clone https://github.com/Michaverse/ai-playground.git
cd ai-playground
```


### 2. Integrate lokal Models

Setup Python environment(s)
> Different local AI Models use different Python versions!
Please follow the installation Instructions for each AI Model

> You will also need to install CUDA
and PyTorch for each venv (Must be matching your CUDA Version / For me: 2.5.1+cu121)


```bash
# Whisper
activate scripts/venv-whisper/scripts/Activate.ps1
source scripts/venv-whisper/bin/activate
pip install -r requirements-whisper.txt

# Chatterbox
activate scripts/venv-chatterbox/scripts/Activate.ps1
source scripts/venv-chatterbox/bin/activate
pip install -r requirements-chatterbox.txt
```


#### Zonos
clone the repo to /repos
Overwrite certain scripts after initialising (see scripts/zonos-edited-scripts)


#### Testing:
```bash
.\scripts\nameOfVenv\Scripts\Activate.ps1
python --version
# Shows Python Version for venv
python -c "import torch; print(torch.version.cuda, torch.cuda.is_available())"
# Shows torch Cuda Version and if it recognized your Graphics Card
```


### 3. Setup `.env` file

For the use of the APIs you will need to have access to Google and OpenAI APIs

Create a `.env` file in the root directory with:
```env
PORT = yourPORT
OPEN_AI_KEY = "yourOpenAIKey"
CAIP_PROJECT_ID = "yourGoogleProjectId"
```


### 4. Launch the tool
```bash
npm install
npm run loop
```

---

## Folder Structure
```bash
ğŸ“ ai-tool
â”œâ”€â”€ readme.md
â”œâ”€â”€ server.js
â”œâ”€â”€ nodemon.json
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ package.json
â”œâ”€â”€ package-lock.json
â”œâ”€â”€ Audiofiles/
â”‚   â””â”€â”€ VoiceSample/
â”œâ”€â”€ Images/
â”œâ”€â”€ Memory/
â”‚   â””â”€â”€ cost/
â”‚       â””â”€â”€ cumulativeCosts.json
â”‚   â””â”€â”€ longTermMemory/
â”‚       â””â”€â”€ longTermMemory.txt
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ fonts/
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ variables.js
â”‚   â”œâ”€â”€ eventListener.js
â”‚   â”œâ”€â”€ sideFunctions.js
â”‚   â”œâ”€â”€ main.js
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ style.css
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ _powerShellScripts/
â”‚   â”œâ”€â”€ venv-chatterbox/
â”‚   â”œâ”€â”€ venv-whisper/
â”‚   â”œâ”€â”€ zonos-edited-Scripts/       // Copy content to "repos/Zonos-for-windows/" after cloning and initiating Zonos
â”‚   â”œâ”€â”€ chatberbotScript.py
â”‚   â”œâ”€â”€ whisperScript.py
â”‚   â”œâ”€â”€ StandardVoiceEnglish.wav
â”‚   â””â”€â”€ StandardVoiceGerman.wav
â”œâ”€â”€ repos/
â”‚   â””â”€â”€ Zonos-for-windows/
â”œâ”€â”€ .git/
â””â”€â”€ node_modules/
```

---

## Possible Roadmap

- [ ] API alternatives of Local Models for faster response time
- [ ] Electron Export
- [ ] Upload Images and Files for LLM
- [ ] Let AI talk to itself
- [ ] Bugfix of Text Formatting when sending to Voice Model

---

## Want to collaborate?

If you're a dev, creator or just curious â€” let's connect!
Feel free to open an issue or write me on [Instagram](https://www.instagram.com/michaverse_youtube/) or [YouTube](https://www.youtube.com/@Michaverse).

---

## License

MIT â€“ free to use, remix, build upon, just credit and donâ€™t be evil. â¤ï¸